\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{hyperref}
\geometry{margin=1in}
\setlist[itemize]{leftmargin=1.4em}
\setlist[enumerate]{leftmargin=1.6em}

\title{University of Tehran -- ECE Department \\
\textbf{Data Science Comprehensive Final Assessment (Extended Edition)}}
\author{Course Staff (Spring 2025) -- Professional Assignment Pack}
\date{Version 1.0}

\begin{document}
\maketitle

\begin{tcolorbox}
\textbf{Course Context:} This extended final integrates the full UT-ECE Data Science track seen in Spring 2024/2025 repositories, from Python and scientific studies to SQL engineering, ML, deep learning, NLP, and LLM agents.
\end{tcolorbox}

\section*{1. Assessment Overview}

\textbf{Primary dataset:} \texttt{GlobalTechTalent\_50k.csv} (50,000 rows). \\
\textbf{Primary target:} \texttt{Migration\_Status} (binary).

\textbf{Additional allowed datasets:}
\begin{itemize}
    \item Course assignment datasets from the UT-ECE repos (where relevant).
    \item Public benchmark datasets with proper citation.
\end{itemize}

\textbf{Required submission artifacts:}
\begin{enumerate}
    \item One reproducible notebook with clear sectioning (Q1--Q20 + Capstone).
    \item One PDF report (max 20 pages excluding appendix).
    \item One code package with scripts/modules and dependency file.
    \item One presentation deck (10--15 slides).
    \item One ethics/fairness memo (1--2 pages).
\end{enumerate}

\textbf{Reproducibility requirements:}
\begin{itemize}
    \item Fix random seeds where applicable.
    \item State train/validation/test splitting strategy.
    \item Log software environment and package versions.
    \item No leakage from post-outcome variables.
\end{itemize}

\section*{2. Grading Distribution (260 points)}

\begin{center}
\begin{tabular}{@{}llc@{}}
\toprule
Block & Focus & Points \\
\midrule
A & Foundations: lifecycle, Python, EDA, scientific studies & 20 \\
B & Inference + visualization design and storytelling & 20 \\
C & SQL engineering + big-data systems thinking & 25 \\
D & Supervised ML + optimization + model selection & 45 \\
E & Unsupervised learning + dimensionality reduction & 20 \\
F & Deep learning + NLP + LMs/LLM agents & 30 \\
G & Ethics, fairness, robustness, governance & 15 \\
H & Integrated capstone implementation + communication & 25 \\
I & Production reliability and advanced diagnostics (Q15--Q20) & 60 \\
J (Bonus) & Advanced research/production extensions (optional) & +20 \\
\midrule
Total (A--I) &  & 260 \\
Optional bonus &  & +20 \\
\bottomrule
\end{tabular}
\end{center}

\section*{Block A -- Foundations (20 points)}

\subsection*{Q1. Data Science Lifecycle and Problem Framing (10 pts)}
Define an end-to-end lifecycle for this migration prediction problem:
\begin{itemize}
    \item business objective and measurable success criteria,
    \item data assumptions and potential failure modes,
    \item deployment setting and monitoring plan.
\end{itemize}

\textbf{Deliverable:} 1-page structured problem statement with a lifecycle diagram.

\subsection*{Q2. Python Data Operations and EDA (10 pts)}
Using Pandas/NumPy/Matplotlib/Seaborn:
\begin{enumerate}
    \item perform robust schema checks (types, nulls, outliers),
    \item produce at least six EDA plots with interpretation,
    \item implement one reusable preprocessing function with tests.
\end{enumerate}

\textbf{Deliverable:} EDA section in notebook + tested utility function.

\section*{Block B -- Inference and Visualization (20 points)}

\subsection*{Q3. Scientific Studies and Inference (10 pts)}
Address the following:
\begin{itemize}
    \item observational vs experimental framing for migration analysis,
    \item sampling bias risks,
    \item one confidence interval and one hypothesis test with assumptions.
\end{itemize}

\subsection*{Q4. Visualization Design + Storytelling (10 pts)}
Create a narrative dashboard (or notebook dashboard section) for non-technical stakeholders.
Must include:
\begin{itemize}
    \item clear KPI definitions,
    \item color and preattentive design rationale,
    \item one misleading-visualization pitfall and correction.
\end{itemize}

\section*{Block C -- SQL and Data Engineering (25 points)}

\subsection*{Q5. SQL-1/SQL-2 Advanced Querying (15 pts)}
Write SQL for:
\begin{enumerate}
    \item 3-year moving average citations by country (window function),
    \item top decile ranking and percentile bucketing,
    \item one CTE-based cohort retention/transition style query.
\end{enumerate}

\subsection*{Q6. Data Leakage and Big-Data Architecture (10 pts)}
\begin{itemize}
    \item Identify leaky features and defend exclusions.
    \item Propose a scalable batch + streaming architecture (Bronze/Silver/Gold or equivalent).
    \item Explain feature store design for training-serving consistency.
\end{itemize}

\section*{Block D -- Supervised Learning and Optimization (45 points)}

\subsection*{Q7. Linear/Logistic Models + Regularization (15 pts)}
\begin{itemize}
    \item Fit baseline linear and logistic models.
    \item Derive and implement Elastic Net objective gradients (or use validated library implementation with derivation in report).
    \item Interpret coefficients, p-values/intervals (where applicable), and calibration.
\end{itemize}

\subsection*{Q8. Optimization Deep Dive (10 pts)}
Compare SGD, Momentum, and Adam on a ravine-style objective.
\begin{itemize}
    \item show trajectories,
    \item discuss curvature and oscillation,
    \item recommend optimizer under feature-scale heterogeneity.
\end{itemize}

\subsection*{Q9. Model Family Comparison (20 pts)}
Train and compare:
\begin{itemize}
    \item SVM/KNN,
    \item Decision Tree/Random Forest,
    \item one boosting model (XGBoost/GradientBoosting/CatBoost if available).
\end{itemize}
Required:
\begin{enumerate}
    \item cross-validation protocol,
    \item hyperparameter search strategy,
    \item error analysis and confusion patterns.
\end{enumerate}

\section*{Block E -- Unsupervised Learning (20 points)}

\subsection*{Q10. Dimensionality Reduction (10 pts)}
\begin{itemize}
    \item PCA with explained variance ratio,
    \item one additional method (random projection, t-SNE, or UMAP),
    \item interpretation of latent dimensions.
\end{itemize}

\subsection*{Q11. Clustering (10 pts)}
\begin{itemize}
    \item K-Means with elbow and silhouette analysis,
    \item DBSCAN (or equivalent density method),
    \item compare cluster stability and practical meaning.
\end{itemize}

\section*{Block F -- Deep Learning, NLP, and LMs (30 points)}

\subsection*{Q12. Neural Networks and Sequence Models (15 pts)}
Complete one tabular NN and one sequence/NLP model experiment:
\begin{itemize}
    \item MLP or shallow feed-forward network for tabular task,
    \item CNN or RNN/LSTM/GRU for text/sequence variant,
    \item compare against classical baseline.
\end{itemize}

\subsection*{Q13. Language Models and LLM Agents (15 pts)}
\begin{itemize}
    \item design a small agentic workflow (retrieval/planning/tool-use pseudocode acceptable),
    \item define evaluation criteria (faithfulness, hallucination rate, safety),
    \item discuss governance constraints for academic/enterprise deployment.
\end{itemize}

\section*{Block G -- Ethics and Governance (15 points)}

\subsection*{Q14. Fairness, Bias, and Responsible Deployment (15 pts)}
\begin{itemize}
    \item evaluate subgroup metrics (e.g., by country/education),
    \item discuss historical-policy bias and proxy discrimination,
    \item propose human-in-the-loop and override policy.
\end{itemize}

\section*{Block H -- Integrated Capstone (25 points)}

\subsection*{Capstone Task}
Deliver a full-stack implementation that includes:
\begin{enumerate}
    \item data preprocessing and leakage-safe training,
    \item model card and experiment tracking summary,
    \item SHAP-based local and global explainability,
    \item deployment recommendation with monitoring thresholds.
\end{enumerate}

\textbf{Required capstone outputs:}
\begin{itemize}
    \item one local explanation for a high-citation candidate predicted as no-migration,
    \item one global feature-importance plot,
    \item one fairness slice table,
    \item one executive summary for non-technical stakeholders.
\end{itemize}

\section*{Block I -- Production Reliability Extension (30 points)}

\subsection*{Q15. Calibration and Threshold Policy (10 pts)}
Using your best supervised model from earlier sections:
\begin{itemize}
    \item generate a reliability/calibration curve,
    \item compute at least one probabilistic calibration metric (e.g., Brier score, ECE),
    \item derive two threshold policies:
    \begin{enumerate}
        \item threshold maximizing F1,
        \item threshold minimizing an asymmetric cost (e.g., FN cost > FP cost).
    \end{enumerate}
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
    \item calibration plot,
    \item threshold-vs-metric tradeoff plot,
    \item final threshold recommendation with justification.
\end{itemize}

\subsection*{Q16. Drift Detection and Monitoring Design (10 pts)}
Define and execute a drift analysis between two data windows (time-based if possible).
\begin{itemize}
    \item compute PSI for numeric features and rank by severity,
    \item include one categorical drift indicator (e.g., JS divergence over country distribution),
    \item propose a monitoring policy (warning/critical thresholds and retraining triggers).
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
    \item drift table (feature, metric, status),
    \item drift ranking figure,
    \item concise monitoring SOP for production.
\end{itemize}

\subsection*{Q17. Counterfactual Recourse Analysis (10 pts)}
For near-boundary negative predictions, estimate minimal actionable changes needed to flip outcome.
\begin{itemize}
    \item choose at least two actionable features (e.g., GitHub activity, citations),
    \item compute minimal intervention per candidate under realistic caps,
    \item report recourse success rate and median intervention magnitude by feature.
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
    \item recourse examples table,
    \item recourse-effort summary plot,
    \item discussion of practicality/ethics of suggested interventions.
\end{itemize}

\subsection*{Q18. Temporal Backtesting and Rolling Validation (10 pts)}
Run a temporal robustness experiment.
\begin{itemize}
    \item Use chronological folds where a valid time column exists.
    \item If no valid time column exists, define and justify a fallback ordering strategy.
    \item Report fold-wise performance (\texttt{AUC}, \texttt{F1}) and decay relative to the first fold.
    \item Add a drift-aware interpretation using a feature drift proxy (e.g., mean PSI).
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
    \item temporal backtest table (\texttt{csv}),
    \item degradation figure (\texttt{png}),
    \item explanation of fallback strategy (if used) in the report.
\end{itemize}

\subsection*{Q19. Uncertainty Quantification and Coverage (10 pts)}
Quantify predictive uncertainty and validate reliability.
\begin{itemize}
    \item Implement conformal or calibrated probability intervals.
    \item Evaluate empirical coverage across multiple confidence levels.
    \item Report interval width and under-coverage risk.
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
    \item coverage summary table (\texttt{csv}),
    \item coverage-vs-confidence figure (\texttt{png}),
    \item policy note for handling low-confidence cases.
\end{itemize}

\subsection*{Q20. Fairness Mitigation Experiment (10 pts)}
Design and evaluate a mitigation intervention under explicit policy constraints.
\begin{itemize}
    \item Train a baseline model and compute subgroup fairness metrics.
    \item Apply one mitigation method (reweighing/thresholding/other justified method).
    \item Compare pre/post fairness and pre/post utility.
    \item Define and evaluate policy constraints (e.g., max acceptable AUC drop).
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
    \item pre/post mitigation comparison table (\texttt{csv}),
    \item fairness-performance tradeoff figure (\texttt{png}),
    \item deployment recommendation based on policy constraints.
\end{itemize}

\section*{Block J -- Advanced Extensions (Bonus +20)}
Any subset earns partial bonus. Keep results reproducible and justified.
\begin{enumerate}
    \item \textbf{Causal framing (5 pts)}: propose a DAG for migration, identify (in)valid adjustment sets, and discuss identifiability limits.
    \item \textbf{Uncertainty (5 pts)}: add conformal prediction or calibrated prediction intervals with empirical coverage check on the test split.
    \item \textbf{Temporal robustness (5 pts)}: perform time-based validation (e.g., train on earlier years, test on later) and compare to random split; report drift-aware degradation.
    \item \textbf{Streaming/online serving (5 pts)}: outline a minimal online inference design (feature freshness, idempotent writes, latency/SLA), plus a guardrail for out-of-distribution detection.
\end{enumerate}

\section*{3. Academic Integrity and Professional Standards}

\begin{itemize}
    \item Cite all external resources and model-generated assistance.
    \item Any copied code without attribution is a violation.
    \item Report negative results honestly.
    \item Prefer interpretable, audited pipelines over leaderboard-only optimization.
\end{itemize}

\section*{4. Bonus (up to +10 points)}

\begin{itemize}
    \item Causal inference extension (DAG + identification discussion).
    \item Real-time pipeline prototype for streaming updates.
    \item Advanced uncertainty quantification (conformal or Bayesian approximation).
\end{itemize}

\end{document}
